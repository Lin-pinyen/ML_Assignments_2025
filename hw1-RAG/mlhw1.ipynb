{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TFwaJir_Olj"
      },
      "source": [
        "# ML2025 Homework 1 - Retrieval Augmented Generation with Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tQHdH2k_Olk"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_ZkNxqGGhdl"
      },
      "source": [
        "First, we will mount your own Google Drive and change the working directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWQh-lq8GuwZ",
        "outputId": "f3b91d6d-9318-4e1a-f722-4e21c654426d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_5Tf1rMHBQ-",
        "outputId": "8c732bc5-9a74-40c2-a46f-5607953107ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '[change to the directory you prefer]'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# Change the working directory to somewhere in your Google Drive.\n",
        "# You could check the path by right clicking on the folder.\n",
        "%cd [change to the directory you prefer]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGx000oZ_Oll"
      },
      "source": [
        "In this section, we install the necessary python packages and download model weights of the quantized version of LLaMA 3.1 8B. Also, download the dataset. Note that the model weight is around 8GB. If you are using your Google Drive as the working directory, make sure you have enough space for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JywoPOO_Oll",
        "outputId": "39e1ab9f-2289-43b8-ac9e-42f3ed92ce0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
            "Collecting llama-cpp-python==0.3.4\n",
            "  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.4-cu122/llama_cpp_python-0.3.4-cp311-cp311-linux_x86_64.whl (445.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m445.2/445.2 MB\u001b[0m \u001b[31m275.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.4) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.4) (2.0.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.3.4)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.4) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.3.4) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m257.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.4\n",
            "Collecting googlesearch-python\n",
            "  Downloading googlesearch_python-1.3.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Collecting requests-html\n",
            "  Downloading requests_html-0.10.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting lxml_html_clean\n",
            "  Downloading lxml_html_clean-0.4.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.9 in /usr/local/lib/python3.11/dist-packages (from googlesearch-python) (4.13.3)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from googlesearch-python) (2.32.3)\n",
            "Collecting pyquery (from requests-html)\n",
            "  Downloading pyquery-2.0.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting fake-useragent (from requests-html)\n",
            "  Downloading fake_useragent-2.1.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting parse (from requests-html)\n",
            "  Downloading parse-1.20.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting w3lib (from requests-html)\n",
            "  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting pyppeteer>=0.0.14 (from requests-html)\n",
            "  Downloading pyppeteer-2.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from lxml_html_clean) (5.3.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.9->googlesearch-python) (4.12.2)\n",
            "Collecting appdirs<2.0.0,>=1.4.3 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: certifi>=2023 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (2025.1.31)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (8.6.1)\n",
            "Collecting pyee<12.0.0,>=11.0.0 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading pyee-11.1.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from pyppeteer>=0.0.14->requests-html) (4.67.1)\n",
            "Collecting urllib3<2.0.0,>=1.25.8 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<11.0,>=10.0 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->googlesearch-python) (3.10)\n",
            "Collecting cssselect>=1.2.0 (from pyquery->requests-html)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.21.0)\n",
            "Downloading googlesearch_python-1.3.0-py3-none-any.whl (5.6 kB)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
            "Downloading lxml_html_clean-0.4.1-py3-none-any.whl (14 kB)\n",
            "Downloading pyppeteer-2.0.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fake_useragent-2.1.0-py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.8/125.8 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parse-1.20.2-py2.py3-none-any.whl (20 kB)\n",
            "Downloading pyquery-2.0.1-py3-none-any.whl (22 kB)\n",
            "Downloading w3lib-2.3.1-py3-none-any.whl (21 kB)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading pyee-11.1.1-py3-none-any.whl (15 kB)\n",
            "Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: parse, appdirs, websockets, w3lib, urllib3, pyee, lxml_html_clean, fake-useragent, cssselect, pyquery, pyppeteer, bs4, requests-html, googlesearch-python\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 14.2\n",
            "    Uninstalling websockets-14.2:\n",
            "      Successfully uninstalled websockets-14.2\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.3.0\n",
            "    Uninstalling urllib3-2.3.0:\n",
            "      Successfully uninstalled urllib3-2.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 1.5.0 requires websockets<15.0dev,>=13.0, but you have websockets 10.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed appdirs-1.4.4 bs4-0.0.2 cssselect-1.3.0 fake-useragent-2.1.0 googlesearch-python-1.3.0 lxml_html_clean-0.4.1 parse-1.20.2 pyee-11.1.1 pyppeteer-2.0.0 pyquery-2.0.1 requests-html-0.10.0 urllib3-1.26.20 w3lib-2.3.1 websockets-10.4\n",
            "--2025-03-24 15:41:19--  https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.202.34, 13.35.202.97, 13.35.202.40, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.202.34|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/d6/e9/d6e9318f285870e2a0e3056e22f9c7ec90cd13e14cfde122129ae66af9ad788f/9da71c45c90a821809821244d4971e5e5dfad7eb091f0b8ff0546392393b6283?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%3B+filename%3D%22Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%22%3B&Expires=1742833317&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MjgzMzMxN319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2Q2L2U5L2Q2ZTkzMThmMjg1ODcwZTJhMGUzMDU2ZTIyZjljN2VjOTBjZDEzZTE0Y2ZkZTEyMjEyOWFlNjZhZjlhZDc4OGYvOWRhNzFjNDVjOTBhODIxODA5ODIxMjQ0ZDQ5NzFlNWU1ZGZhZDdlYjA5MWYwYjhmZjA1NDYzOTIzOTNiNjI4Mz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=tW29xcF9XRl7K%7Ebou-19KlMhF6WbAc2AtbOng1gnCiU142Ad42jKs1EnjIIHuPydMfv8dL0L7cRRUshjlj-XGSEBTbTdVtqPPzToX4CbbDuHwpwRsZWH8Qe1O42fQ0Kk04Q6UeqGmac7v5zF09mKPLtKUNJ4FHDwa2TDW0JyPz5F%7EGkmGDJ2Gn3zBOJIPCHGw0i4SjhoslDQEzh%7E219Ys8PrPqak1ngZLiJ9jRAVAirYP8jf-7WTPP-aHpJ6WJcNPqc920OqKWkfLAQmBbL7ggSeqWpAbY2UA0tG3bL7oug-1Bo34z45ggrtKb0-1ynkr1mSS4UYHoDBDoA%7Ejt9NGQ__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-03-24 15:41:20--  https://cdn-lfs-us-1.hf.co/repos/d6/e9/d6e9318f285870e2a0e3056e22f9c7ec90cd13e14cfde122129ae66af9ad788f/9da71c45c90a821809821244d4971e5e5dfad7eb091f0b8ff0546392393b6283?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%3B+filename%3D%22Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%22%3B&Expires=1742833317&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MjgzMzMxN319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2Q2L2U5L2Q2ZTkzMThmMjg1ODcwZTJhMGUzMDU2ZTIyZjljN2VjOTBjZDEzZTE0Y2ZkZTEyMjEyOWFlNjZhZjlhZDc4OGYvOWRhNzFjNDVjOTBhODIxODA5ODIxMjQ0ZDQ5NzFlNWU1ZGZhZDdlYjA5MWYwYjhmZjA1NDYzOTIzOTNiNjI4Mz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=tW29xcF9XRl7K%7Ebou-19KlMhF6WbAc2AtbOng1gnCiU142Ad42jKs1EnjIIHuPydMfv8dL0L7cRRUshjlj-XGSEBTbTdVtqPPzToX4CbbDuHwpwRsZWH8Qe1O42fQ0Kk04Q6UeqGmac7v5zF09mKPLtKUNJ4FHDwa2TDW0JyPz5F%7EGkmGDJ2Gn3zBOJIPCHGw0i4SjhoslDQEzh%7E219Ys8PrPqak1ngZLiJ9jRAVAirYP8jf-7WTPP-aHpJ6WJcNPqc920OqKWkfLAQmBbL7ggSeqWpAbY2UA0tG3bL7oug-1Bo34z45ggrtKb0-1ynkr1mSS4UYHoDBDoA%7Ejt9NGQ__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 54.230.71.17, 54.230.71.70, 54.230.71.110, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|54.230.71.17|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8540775840 (8.0G) [binary/octet-stream]\n",
            "Saving to: ‘Meta-Llama-3.1-8B-Instruct-Q8_0.gguf’\n",
            "\n",
            "Meta-Llama-3.1-8B-I 100%[===================>]   7.95G   101MB/s    in 91s     \n",
            "\n",
            "2025-03-24 15:42:51 (89.4 MB/s) - ‘Meta-Llama-3.1-8B-Instruct-Q8_0.gguf’ saved [8540775840/8540775840]\n",
            "\n",
            "--2025-03-24 15:42:51--  https://www.csie.ntu.edu.tw/~ulin/public.txt\n",
            "Resolving www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)... 140.112.30.26\n",
            "Connecting to www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)|140.112.30.26|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4399 (4.3K) [text/plain]\n",
            "Saving to: ‘public.txt’\n",
            "\n",
            "public.txt          100%[===================>]   4.30K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-03-24 15:42:51 (251 MB/s) - ‘public.txt’ saved [4399/4399]\n",
            "\n",
            "--2025-03-24 15:42:51--  https://www.csie.ntu.edu.tw/~ulin/private.txt\n",
            "Resolving www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)... 140.112.30.26\n",
            "Connecting to www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)|140.112.30.26|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15229 (15K) [text/plain]\n",
            "Saving to: ‘private.txt’\n",
            "\n",
            "private.txt         100%[===================>]  14.87K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-03-24 15:42:52 (258 KB/s) - ‘private.txt’ saved [15229/15229]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pip install --no-cache-dir llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
        "!python3 -m pip install googlesearch-python bs4 charset-normalizer requests-html lxml_html_clean\n",
        "\n",
        "from pathlib import Path\n",
        "if not Path('./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf').exists():\n",
        "    !wget https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\n",
        "if not Path('./public.txt').exists():\n",
        "    !wget https://www.csie.ntu.edu.tw/~ulin/public.txt\n",
        "if not Path('./private.txt').exists():\n",
        "    !wget https://www.csie.ntu.edu.tw/~ulin/private.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX6SizAt_Olm",
        "outputId": "05c9de23-3a38-40f4-9046-1448bb871476"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are good to go!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "if not torch.cuda.is_available():\n",
        "    raise Exception('You are not using the GPU runtime. Change it first or you will suffer from the super slow inference speed!')\n",
        "else:\n",
        "    print('You are good to go!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3iyc1qC_Olm"
      },
      "source": [
        "## Prepare the LLM and LLM utility function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T59vxAo2_Olm"
      },
      "source": [
        "By default, we will use the quantized version of LLaMA 3.1 8B. you can get full marks on this homework by using the provided LLM and LLM utility function. You can also try out different LLM models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtepTeT3_Olm"
      },
      "source": [
        "In the following code block, we will load the downloaded LLM model weights onto the GPU first.\n",
        "Then, we implemented the generate_response() function so that you can get the generated response from the LLM model more easily."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVil2Vhe_Olm"
      },
      "source": [
        "You can ignore \"llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\" warning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScyW45N__Olm",
        "outputId": "54567a27-1323-4696-bc08-c89a70ec0543"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        }
      ],
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "# Load the model onto GPU\n",
        "llama3 = Llama(\n",
        "    \"./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\",\n",
        "    verbose=False,\n",
        "    n_gpu_layers=-1, #確保每一層都用GPU\n",
        "    n_ctx=16384,    # This argument is how many tokens the model can take. The longer the better, but it will consume more memory. 16384 is a proper value for a GPU with 16GB VRAM.\n",
        ")\n",
        "\n",
        "def generate_response(_model: Llama, _messages: str) -> str:\n",
        "    '''\n",
        "    This function will inference the model with given messages.\n",
        "    '''\n",
        "    _output = _model.create_chat_completion(\n",
        "        _messages,\n",
        "        stop=[\"<|eot_id|>\", \"<|end_of_text|>\"],\n",
        "        max_tokens=512,    # This argument is how many tokens the model can generate, you can change it and observe the differences.\n",
        "        temperature=0,      # This argument is the randomness of the model. 0 means no randomness. You will get the same result with the same input every time. You can try to set it to different values.\n",
        "        repeat_penalty=2.0,\n",
        "    )[\"choices\"][0][\"message\"][\"content\"]\n",
        "    return _output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnHLwq-4_Olm"
      },
      "source": [
        "## Search Tool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYM-2ZsE_Olm"
      },
      "source": [
        "The TA has implemented a search tool for you to search certain keywords using Google Search. You can use this tool to search for the relevant **web pages** for the given question. The search tool can be integrated in the following sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bEIRmZl7_Oln"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from googlesearch import search as _search\n",
        "from bs4 import BeautifulSoup\n",
        "from charset_normalizer import detect\n",
        "import asyncio\n",
        "from requests_html import AsyncHTMLSession\n",
        "import urllib3\n",
        "import spacy\n",
        "\n",
        "urllib3.disable_warnings()\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "async def worker(s:AsyncHTMLSession, url:str):\n",
        "    try:\n",
        "        header_response = await asyncio.wait_for(s.head(url, verify=False), timeout=10)\n",
        "        if 'text/html' not in header_response.headers.get('Content-Type', ''):\n",
        "            return None\n",
        "        r = await asyncio.wait_for(s.get(url, verify=False), timeout=10)\n",
        "        return r.text\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "async def get_htmls(urls):\n",
        "    session = AsyncHTMLSession()\n",
        "    tasks = (worker(session, url) for url in urls)\n",
        "    return await asyncio.gather(*tasks)\n",
        "\n",
        "async def search(keyword: str, n_results: int=3) -> List[str]:\n",
        "    '''\n",
        "    This function will search the keyword and return the text content in the first n_results web pages.\n",
        "\n",
        "    Warning: You may suffer from HTTP 429 errors if you search too many times in a period of time. This is unavoidable and you should take your own risk if you want to try search more results at once.\n",
        "    The rate limit is not explicitly announced by Google, hence there's not much we can do except for changing the IP or wait until Google unban you (we don't know how long the penalty will last either).\n",
        "    '''\n",
        "    keyword = keyword[:100]\n",
        "    # First, search the keyword and get the results. Also, get 2 times more results in case some of them are invalid.\n",
        "    results = list(_search(keyword, n_results * 2, lang=\"zh\", unique=True))\n",
        "    # Then, get the HTML from the results. Also, the helper function will filter out the non-HTML urls.\n",
        "    results = await get_htmls(results)\n",
        "    # Filter out the None values.\n",
        "    results = [x for x in results if x is not None]\n",
        "    # Parse the HTML.\n",
        "    results = [BeautifulSoup(x, 'html.parser') for x in results]\n",
        "    # Get the text from the HTML and remove the spaces. Also, filter out the non-utf-8 encoding.\n",
        "    results = [''.join(x.get_text().split()) for x in results if detect(x.encode()).get('encoding') == 'utf-8']\n",
        "    # Return the first n results.\n",
        "    return results[:n_results]\n",
        "# 關鍵字提取：這裡使用 spaCy NLP 來提取問題中的關鍵字\n",
        "def extract_keywords(question: str) -> str:\n",
        "    doc = nlp(question)\n",
        "    keywords = [token.text for token in doc if token.pos_ in [\"NOUN\", \"PROPN\", \"ADJ\"]]\n",
        "    return \" \".join(keywords)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC3zQjjj_Oln"
      },
      "source": [
        "## Test the LLM inference pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dmGCARd_Oln",
        "outputId": "9a09fc69-f1e1-4f17-cebe-66010d9f32dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "泰勒絲（Taylor Swift）是一位美國歌手、詞曲作家和音樂製作人。她出生於1989年，來自田納西州。她的音乐风格从乡村摇滚发展到流行搖擺，並且她被誉为当代最成功的女艺人的之一。\n",
            "\n",
            "泰勒絲早期在鄉郊小鎮演唱會時開始發展音樂事業，她推出了多張專輯，包括《Taylor Swift》、《Fearless》，以及後來更為知名的大熱作如 《1989》（2014年）、_reputation（）和 _Lover （）。她的歌曲經常探討愛情、友誼及自我成長等主題。\n",
            "\n",
            "泰勒絲獲得了許多獎項，包括13座格萊美奖，並且是史上最快達到百萬銷量的女藝人之一。\n"
          ]
        }
      ],
      "source": [
        "# You can try out different questions here.\n",
        "test_question='請問誰是 Taylor Swift？'\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文時只會使用繁體中文來回問題。\"},    # System prompt\n",
        "    {\"role\": \"user\", \"content\": test_question}, # User prompt\n",
        "]\n",
        "\n",
        "print(generate_response(llama3, messages))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0-ojJuE_Oln"
      },
      "source": [
        "## Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGsIPud3_Oln"
      },
      "source": [
        "The TA has implemented the Agent class for you. You can use this class to create agents that can interact with the LLM model. The Agent class has the following attributes and methods:\n",
        "- Attributes:\n",
        "    - role_description: The role of the agent. For example, if you want this agent to be a history expert, you can set the role_description to \"You are a history expert. You will only answer questions based on what really happened in the past. Do not generate any answer if you don't have reliable sources.\".\n",
        "    - task_description: The task of the agent. For example, if you want this agent to answer questions only in yes/no, you can set the task_description to \"Please answer the following question in yes/no. Explanations are not needed.\"\n",
        "    - llm: Just an indicator of the LLM model used by the agent.\n",
        "- Method:\n",
        "    - inference: This method takes a message as input and returns the generated response from the LLM model. The message will first be formatted into proper input for the LLM model. (This is where you can set some global instructions like \"Please speak in a polite manner\" or \"Please provide a detailed explanation\".) The generated response will be returned as the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "zjG-UwDX_Oln"
      },
      "outputs": [],
      "source": [
        "class LLMAgent():\n",
        "    def __init__(self, role_description: str, task_description: str, llm:str=\"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\"):\n",
        "        self.role_description = role_description   # Role means who this agent should act like. e.g. the history expert, the manager......\n",
        "        self.task_description = task_description    # Task description instructs what task should this agent solve.\n",
        "        self.llm = llm  # LLM indicates which LLM backend this agent is using.\n",
        "    def inference(self, message:str) -> str:\n",
        "        if self.llm == 'bartowski/Meta-Llama-3.1-8B-Instruct-GGUF': # If using the default one.\n",
        "            # TODO: Design the system prompt and user prompt here.\n",
        "            # Format the messsages first.\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": f\"{self.role_description}\"},  # Hint: you may want the agents to speak Traditional Chinese only.\n",
        "                {\"role\": \"user\", \"content\": f\"{self.task_description}\\n{message}\"}, # Hint: you may want the agents to clearly distinguish the task descriptions and the user messages. A proper seperation text rather than a simple line break is recommended.\n",
        "            ]\n",
        "            return generate_response(llama3, messages)\n",
        "        else:\n",
        "            # TODO: If you want to use LLMs other than the given one, please implement the inference part on your own.\n",
        "            return \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-ueJrgP_Oln"
      },
      "source": [
        "TODO: Design the role description and task description for each agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzPzmNnj_Oln"
      },
      "outputs": [],
      "source": [
        "# TODO: Design the role and task description for each agent.\n",
        "\n",
        "# # Question extraction agent - designed to extract the core question\n",
        "# question_extraction_agent = LLMAgent(\n",
        "#     role_description=\"你是一個專精於理解問題的AI助手。你的任務是從用戶輸入中提取核心問題，去除無關的資訊，且核心問題需要為一個完整的問句，不能只是單一的名詞或形容詞。使用中文時只會使用繁體中文回應。\",\n",
        "#     task_description=\"請從以下輸入中提取核心問題，忽略任何與問題無關的內容。只返回提取出的核心問題，不要添加任何解釋或額外資訊：\",\n",
        "# )\n",
        "\n",
        "# # Keyword extraction agent - designed to identify optimal search keywords\n",
        "# keyword_extraction_agent = LLMAgent(\n",
        "#     role_description=\"你是一個專精於提取搜尋關鍵字的AI助手。你的任務是從問題中識別出最關鍵、最具體的詞語，這些詞語能夠用於精確的網路搜尋。使用中文時只會使用繁體中文回應。\",\n",
        "#     task_description=\"請從以下問題中提取關鍵詞或短語，這些關鍵詞應該是問題中最獨特、最具體的部分。請嚴格遵守以下規則：\\n1. 只返回關鍵詞，不要包含任何解釋\\n2. 關鍵詞之間用空格分隔\\n3. 不要使用標點符號\\n4. 不要加入你認為相關但問題中沒有的詞語：\",\n",
        "# )\n",
        "# 問題提煉 agent：負責從用戶輸入中提取核心問題，去除冗餘資訊\n",
        "question_extraction_agent = LLMAgent(\n",
        "    role_description=\"你是一個問題提煉專家，專門從冗長或複雜的描述中提煉出核心問題，並用精簡語言呈現。\",\n",
        "    task_description=\"請閱讀下面的輸入，並提煉出最核心、最直接需要回答的問題，這個問題為一個完整的句子。請保持輸出簡明扼要，不要包含額外描述或背景資訊。看到「」可以把裡面的內容當作是核心問題\"\n",
        ")\n",
        "\n",
        "# 關鍵字提取 agent：負責從問題中抽取出最具代表性的關鍵字，供後續搜索使用\n",
        "keyword_extraction_agent = LLMAgent(\n",
        "    role_description=\"你是一個專精於提取搜尋關鍵字的AI助手。你的任務是從問題中識別出最關鍵、最具體的詞語，這些詞語能夠用於精確的網路搜尋，並且這些詞語必須是繁體中文。\",\n",
        "    task_description=\"請從以下問題中提取關鍵詞或短語，這些關鍵詞應該是問題中最獨特、最具體的部分。特別注意地點和形容詞和動詞。請嚴格遵守以下規則：\\n1. 只返回關鍵詞，不要包含任何解釋\\n2. 關鍵詞之間用空格分隔\\n3. 不要使用標點符號\\n4. 不要加入你認為相關但問題中沒有的詞語：\"\n",
        ")\n",
        "# This agent is the core component that answers the question.\n",
        "qa_agent = LLMAgent(\n",
        "    role_description=\"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文時只會使用繁體中文來回問題。\",\n",
        "    task_description=\"請回答以下問題：\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9eoywr7_Oln"
      },
      "source": [
        "## RAG pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HDOjNYJ_Oln"
      },
      "source": [
        "TODO: Implement the RAG pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRGNa-1i_Oln"
      },
      "source": [
        "Please refer to the homework description slides for hints.\n",
        "\n",
        "Also, there might be more heuristics (e.g. classifying the questions based on their lengths, determining if the question need a search or not, reconfirm the answer before returning it to the user......) that are not shown in the flow charts. You can use your creativity to come up with a better solution!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMaIsKAZ_Olo"
      },
      "source": [
        "- Naive approach (simple baseline)\n",
        "\n",
        "    ![](https://www.csie.ntu.edu.tw/~ulin/naive.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mppO-oOO_Olo"
      },
      "source": [
        "- Naive RAG approach (medium baseline)\n",
        "\n",
        "    ![](https://www.csie.ntu.edu.tw/~ulin/naive_rag.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYxbciLO_Olo"
      },
      "source": [
        "- RAG with agents (strong baseline)\n",
        "\n",
        "    ![](https://www.csie.ntu.edu.tw/~ulin/rag_agent.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztJkA7R7_Olo"
      },
      "outputs": [],
      "source": [
        "async def pipeline(question: str) -> str:\n",
        "    \"\"\"\n",
        "    Implementation of the RAG with agents pipeline:\n",
        "    1. Extract the core question using the question extraction agent\n",
        "    2. Extract search keywords using the keyword extraction agent\n",
        "    3. Retrieve relevant information using the search tool\n",
        "    4. Generate an answer using the QA agent with the retrieved information\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Step 1: Extract the core question\n",
        "        extracted_question = question_extraction_agent.inference(question)\n",
        "        print(f\"Extracted question: {extracted_question}\")\n",
        "\n",
        "        # # Step 2: Extract keywords for search\n",
        "        keywords = keyword_extraction_agent.inference(extracted_question)\n",
        "        print(f\"Search keywords: {keywords}\")\n",
        "\n",
        "        # Step 3: Search for relevant information\n",
        "        search_results = await search(keywords, n_results=3)\n",
        "\n",
        "        # Prepare the context for the QA agent\n",
        "        context = \"\"\n",
        "        if search_results and len(search_results) > 0:\n",
        "            # Limit the context length to avoid exceeding the model's context window\n",
        "            for i, result in enumerate(search_results):\n",
        "                # Truncate each result to avoid extremely long contexts\n",
        "                truncated_result = result[:3000] if len(result) > 3000 else result\n",
        "                context += f\"搜尋結果 {i+1}:\\n{truncated_result}\\n\\n\"\n",
        "\n",
        "        # Step 4: Formulate a prompt that includes both the original question and search results\n",
        "        final_prompt = f\"\"\"\n",
        "        問題: {extracted_question}\n",
        "        參考資料:\n",
        "        {context}\n",
        "\n",
        "        基於上述參考資料，請回答問題。如果參考資料中沒有足夠的資訊，請根據你所知道的知識進行回答。請直截回答答案，而不要先說根據什麼資料\n",
        "        \"\"\"\n",
        "\n",
        "        # Step 5: Generate the final answer\n",
        "        answer = qa_agent.inference(final_prompt)\n",
        "\n",
        "        return answer\n",
        "\n",
        "    except Exception as e:\n",
        "        # Fallback to direct QA if any part of the pipeline fails\n",
        "        print(f\"Pipeline error: {e}. Falling back to direct QA.\")\n",
        "        return qa_agent.inference(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_kI_9EGB0S9"
      },
      "source": [
        "## Answer the questions using your pipeline!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN17sSZ8DUg7"
      },
      "source": [
        "Since Colab has usage limit, you might encounter the disconnections. The following code will save your answer for each question. If you have mounted your Google Drive as instructed, you can just rerun the whole notebook to continue your process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plUDRTi_B39S",
        "outputId": "912b1d2e-c18f-4a17-8a16-f7b05014dfa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted question: 「虎山雄風飛揚」是哪間學校的校歌？\n",
            "Search keywords: 虎山雄風飛揚 校歌\n",
            "1 光華國小的校歌是「虎山雄風飛揚」。\n",
            "Extracted question: 民眾透過境外郵購自用產品回台加收審查費多少錢？\n",
            "Search keywords: 境外郵購 自用產品 審查費\n",
            "2 民眾透過境外郵購自用產品回台加收審查費 750 元。\n",
            "Extracted question: 史蒂夫·乔布斯\n",
            "Search keywords: 史蒂夫·乔布斯\n",
            "3 史蒂夫·乔布斯（Steven Paul Jobs）是一名美国企业家、营销人士和发明者。他于1955年2月24日出生，2011 年10 月 ５ 日去世。 他是苹果公司的联合创始之一，并曾担任董事长及首席执行官职位。  他以推动个人电脑革命以及设计创新而闻名，他在Apple和Pixar等多家企业中发挥了重要作用。他也是一个有争议的人物，受到过政府调查并与Adobe公司发生冲突。\n",
            "Extracted question: 托福網路測驗 TOEFL iBT 要達到多少分才能申請進階英文免修？\n",
            "Search keywords: 托福網路測驗 TOEFL iBT 免修分數\n",
            "4 托福網路測驗（TOEFL iBT）要達到 92 分才能申請進階英文免修。\n",
            "Extracted question: 觸地 try 可得 5 分。\n",
            "Search keywords: 觸地 try 得分\n",
            "5 觸地得分（Try）可得到 5 分。\n",
            "Extracted question: 卑南族的祖先發源地是哪裡？\n",
            "Search keywords: 卑南族祖先發源地\n",
            "6 卑南族的祖先發源地是台東太麻里鄉美和海岸附近。\n",
            "Extracted question: 熊仔的碩班指導教授為？\n",
            "Search keywords: 熊仔 碩班 指導教授\n",
            "7 熊仔的碩班指導教授為李琳山。\n",
            "Extracted question: 誰發現了電磁感應定律？\n",
            "Search keywords: 麥克斯韋\n",
            "8 詹姆斯·克拉 克麦克斯韦\n",
            "Extracted question: 距離國立臺灣史前文化博物館最近的臺鐵車站為？\n",
            "Search keywords: 國立臺灣史前文化博物館  臺鐵車站\n",
            "9 康樂車站\n",
            "Extracted question: 20+30=?\n",
            "Search keywords: 20 30 等式\n",
            "10 20+30=50\n",
            "Extracted question: 達拉斯獨行俠隊的Luka Doncic被交易至哪一聯盟中的球队？\n",
            "Search keywords: 達拉斯  獨行俠隊 Luka Doncic交易\n",
            "11 達拉斯獨行俠隊的Luka Doncic被交易至洛杉磯湖人。\n",
            "Extracted question: 2024年美國總統大選的勝选人是誰？\n",
            "Search keywords: 2024年 美國 總統 大選\n",
            "12 唐納·川普\n",
            "Extracted question: Llama-3.2 系列模型中，參數量最小的モデル是多少 Billion 的 parameters？\n",
            "Search keywords: Llama 3.2 模型 參數量 最小\n",
            "13 參數量最小的Llama-3.2系列模型是1B。\n",
            "Extracted question: 學生每個学期最多停修幾門課程？\n",
            "Search keywords: 學生 每個 學期 最多 停修 幾門 課程\n",
            "14 根據國立臺灣大學的相關規定，學生每個学期最多停修兩門課程。\n",
            "Extracted question: DeepSeek公司的母 公司是？\n",
            "Search keywords: DeepSeek 公司\n",
            "15 杭州深度求索人工智慧基础技术研究有限公司（DeepSeek）的母公司是中资对冲基金幻方量化。\n",
            "Extracted question: 2024年NBA的總冠軍隊伍是哪一队？\n",
            "Search keywords: 2024年 NBA 总冠军\n",
            "16 波士顿凯尔特人队是2024年NBA总冠军。\n",
            "Extracted question: 三鍵碳氫化合物\n",
            "Search keywords: 三鍵碳氫化合物\n",
            "17 烃是指仅由碳和氢组成的有机化合物，包括各种类型，如：  1. 烷类（Saturated hydrocarbons） 2.. 弱性键或多重鍵 3.F芳香环  炔是一類不饑含三個雙結構鏈。\n",
            "Extracted question: 阿倫·圖靈\n",
            "Search keywords: 阿倫·圖靈\n",
            "18 艾伦·图灵（Alan Turing）是一位英国计算机科学家、数学学者和逻辑學家的。他被誉为計算機科学生涯的先驱，提出了“圖靈測試”来评估人工智能。\n",
            "Extracted question: 臺灣玄天上帝信仰的進香中心位於宜蘭縣礁溪鄉。\n",
            "Search keywords: 臺灣玄天上帝信仰礁溪鄉宜蘭縣進香中心\n",
            "19 根據參考資料，臺灣玄天上帝信仰的進香中心位於宜蘭縣礁溪鄉，但不是松柏嶺受 天宮，而是勅建 社 溪 協 田 廟。\n",
            "Extracted question: 微軟\n",
            "Search keywords: 微軟\n",
            "20 微軟是一家美國跨國科技公司，成立於1975年。它的產品包括Windows作業系統、Office辦公套件和Xbox遊戲主機等。此外，它還提供雲計算服務Microsoft Azure，以及人工智能技術Copilot。  其企業文化以創新為核心，並且重視員工作場所舒適度與多樣性。在組織結構上，微軟分別有三個主要部門：產品和生產力、商業及營銷以及娛樂事業。\n",
            "Extracted question: 官將首起源自哪間廟宇？\n",
            "Search keywords: 官將\n",
            "21 官將首的起源自新北市 新莊地藏庵。\n",
            "Extracted question: 《咒》的邪神名為？\n",
            "Search keywords: 咒邪神\n",
            "22 《咒》的邪神名為「大黑佛母」。\n",
            "Extracted question: 短暫交會的旅程就此分岔\n",
            "Search keywords: 短暫 交會 旅程 分岔\n",
            "23 短暫交會的旅程就此分岔。\n",
            "Extracted question: 2025 卑南族聯合年聚在哪個部落舉辦？\n",
            "Search keywords: 卑南族 聯合年聚\n",
            "24 2025年卑南族聯合年的聚會將在利嘉部落舉辦。\n",
            "Extracted question: GeForce RTX 多少系列\n",
            "Search keywords: GeForce RTX 多少系列\n",
            "25 GeForce RTX系列共有多個子代，包括：  * GeForce 50 系列 \t+ NVIDIA GeForeRTXTi60（未知）     + Nvidia Geforce Rtx Ti70       *Nvidia geforcer tx ti80*    - Nvidiageforce rxtx90  GeForce RTX40系列： - nVidia ge force rt x 4090 super * GeForce GTX408 SUPER ( 未確認 ) *NVIDIA GeForeRTXTi60*      Nvidia Geforce Rtx Ti70       *Nvidia geforcer tx ti80*    - Nvidiageforce rxtx90  GeForce RTX30系列： - nVidia ge force rt x 3090ti * GeForce GTX308 SUPER ( 未確認 ) *NVIDIA GeForeRTXTi60*      Nvidia Geforce Rtx Ti70       *Nvidia geforcer tx ti80*    - Nvidiageforce rxtx90\n",
            "Extracted question: 大S是在日本旅遊時因病去世。\n",
            "Search keywords: 大S 日本旅遊 病去世\n",
            "26 大S是在日本旅游時因流感并发肺炎去世，享年48岁。\n",
            "Extracted question: 誰發現了萬有引力？\n",
            "Search keywords: 艾薩克·牛頓\n",
            "27 艾萨克·牛顿\n",
            "Extracted question: 台鵠開示計畫「TAIHUCAIS」的英文全名\n",
            "Search keywords: 台鵠開示計畫 TAIHUCAIS\n",
            "28 TAIHUCAIS 的英文全名是 TAIwanHUmanitiesConversational AI Knowledge Discovery System。\n",
            "Extracted question: 《終極殺機》\n",
            "Search keywords: 終極殺機\n",
            "29 《終極殺機》是一部恐怖電影，故事圍繞著一名失去記憶的女性莎莫，她開始察覺到自己被追捕，並且身邊的人是否真的是朋友。\n",
            "Extracted question: H2O\n",
            "Search keywords: H2O\n",
            "30 H2O 是水的化學符號，代表每個分子中有兩顆氫原子的氧氣。\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Fill in your student ID first.\n",
        "STUDENT_ID = \"test-4\"\n",
        "\n",
        "STUDENT_ID = STUDENT_ID.lower()\n",
        "with open('./public.txt', 'r') as input_f:\n",
        "    questions = input_f.readlines()\n",
        "    questions = [l.strip().split(',')[0] for l in questions]\n",
        "    for id, question in enumerate(questions, 1):\n",
        "        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n",
        "            continue\n",
        "        answer = await pipeline(question)\n",
        "        answer = answer.replace('\\n',' ')\n",
        "        print(id, answer)\n",
        "        with open(f'./{STUDENT_ID}_{id}.txt', 'w') as output_f:\n",
        "            print(answer, file=output_f)\n",
        "\n",
        "# with open('./private.txt', 'r') as input_f:\n",
        "#     questions = input_f.readlines()\n",
        "#     for id, question in enumerate(questions, 31):\n",
        "#         if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n",
        "#             continue\n",
        "#         answer = await pipeline(question)\n",
        "#         answer = answer.replace('\\n',' ')\n",
        "#         print(id, answer)\n",
        "#         with open(f'./{STUDENT_ID}_{id}.txt', 'a') as output_f:\n",
        "#             print(answer, file=output_f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "GmLO9PlmEBPn"
      },
      "outputs": [],
      "source": [
        "# Combine the results into one file.\n",
        "with open(f'./{STUDENT_ID}.txt', 'w') as output_f:\n",
        "    for id in range(1,91):\n",
        "        with open(f'./{STUDENT_ID}_{id}.txt', 'r') as input_f:\n",
        "            answer = input_f.readline().strip()\n",
        "            print(answer, file=output_f)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
